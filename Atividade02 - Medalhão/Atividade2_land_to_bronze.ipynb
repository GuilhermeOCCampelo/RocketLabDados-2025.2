{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "936b817e-5084-4448-a6c3-b59449de03d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS medalhao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d27c1993-9b9d-4f39-9dab-ffff6a4431cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG medalhao;\n",
    "    \n",
    "CREATE SCHEMA IF NOT EXISTS bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS silver;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a140e959-37a0-4ed6-9f59-b474338341c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE SCHEMA default; \n",
    "CREATE VOLUME IF NOT EXISTS landing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "124e4258-b7f3-4593-be1d-f790c0f046a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Configuração e Função de Ingestão (CSV)\n",
    "\n",
    "Esta célula define a função `ingest_csv`, que é o núcleo da nossa ingestão da Landing para a Bronze.\n",
    "\n",
    "A função realiza as seguintes etapas:\n",
    "1.  Recebe o nome do arquivo de origem (CSV) e o nome da tabela de destino (Delta).\n",
    "2.  Lê o arquivo CSV do volume `/Volumes/medalhao/default/landing/`, utilizando `header=True` e `inferSchema=True` para detectar automaticamente os tipos de dados.\n",
    "3.  Valida se o DataFrame não está vazio.\n",
    "4.  **Rastreabilidade:** Adiciona a coluna `data_ingestao` com o timestamp atual (`F.current_timestamp()`).\n",
    "5.  Salva os dados brutos no formato Delta na camada `bronze`, usando `mode(\"overwrite\")`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2b9b8e3-b637-4f1a-81ab-f885b8d9ceec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Definição do caminho do schema Bronze\n",
    "catalogo = \"medalhao\"\n",
    "bronze_db_name = \"bronze\"\n",
    "\n",
    "def ingest_csv(nome_arquivo, nome_tabela):\n",
    "   \n",
    "    try:\n",
    "        table_name = nome_tabela\n",
    "        landing_path = f\"/Volumes/medalhao/default/landing/{nome_arquivo}\"\n",
    "\n",
    "        # Leitura do arquivo CSV\n",
    "        df = spark.read.csv(landing_path, header=True, inferSchema=True)\n",
    "\n",
    "        # Validação: arquivo vazio\n",
    "        if df.count() == 0:\n",
    "            raise ValueError(f\"O arquivo {nome_arquivo} está vazio ou não pôde ser lido.\")\n",
    "\n",
    "        # Adiciona timestamp de ingestão\n",
    "        df_with_metadata = df.withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "\n",
    "        # Escrita no formato Delta\n",
    "        df_with_metadata.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalogo}.{bronze_db_name}.{table_name}\")\n",
    "\n",
    "        print(f\"✅ Tabela bronze.{nome_tabela} criada com sucesso!\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar {nome_tabela}: {str(e)}\")\n",
    "#ver se dá pra passar um dicionário depois\n",
    "ingest_csv(\"olist_customers_dataset.csv\",\"ft_consumidores\")\n",
    "ingest_csv(\"olist_geolocation_dataset.csv\",\"ft_geolocalizacao\")\n",
    "ingest_csv(\"olist_order_items_dataset.csv\",\"ft_itens_pedidos\")\n",
    "ingest_csv(\"olist_order_payments_dataset.csv\",\"ft_pagamentos_pedidos\")\n",
    "ingest_csv(\"olist_order_reviews_dataset.csv\",\"ft_avaliacoes_pedidos\")\n",
    "ingest_csv(\"olist_orders_dataset.csv\",\"ft_pedidos\")\n",
    "ingest_csv(\"olist_products_dataset.csv\",\"ft_produtos\")\n",
    "ingest_csv(\"olist_sellers_dataset.csv\",\"ft_vendedores\")\n",
    "ingest_csv(\"product_category_name_translation.csv\",\"dm_categoria_produtos_traducao\")\n",
    "\n",
    "print(\"ingestão concluída\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4abae3cf-68d7-439a-a74d-167a6526be64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Ingestão de Fonte Externa (API - Cotação do Dólar)\n",
    "\n",
    "Esta célula consome uma fonte de dados externa para enriquecer o dataset.\n",
    "1.  Conecta-se ao endpoint da API do Banco Central do Brasil (BCB) para buscar a cotação do dólar .\n",
    "2.  Foi definido um intervalo de datas amplo (2015-2024) para garantir a cobertura de todo o histórico de pedidos.\n",
    "3.  Os dados JSON brutos retornados pela API são carregados em um DataFrame Spark e persistidos na tabela `bronze.dm_cotacao_dolar`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8815e11-2858-4b44-8298-e1019b58c2d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "data_inicio_formatada = \"01-01-2015\"\n",
    "data_fim_formatada = \"12-31-2024\" \n",
    "\n",
    "endpoint = f\"https://olinda.bcb.gov.br/olinda/servico/PTAX/versao/v1/odata/CotacaoDolarPeriodo(dataInicial=@dataInicial,dataFinalCotacao=@dataFinalCotacao)?@dataInicial='{data_inicio_formatada}'&@dataFinalCotacao='{data_fim_formatada}'&$select=dataHoraCotacao,cotacaoCompra&$format=json\"\n",
    "\n",
    "\n",
    "dados_json = requests.get(endpoint).json()\n",
    "\n",
    "#print(dados_json), utilizei o print pra entender como estava sendo retornado esse dicionário, então como value está retornando uma lista com os dados de cotação e data/hora de cada uma delas, utilizei como parametro na hora de criar um datraframe para organizar de maneira ordenada\n",
    "\n",
    "df_spark = spark.createDataFrame(dados_json['value'])\n",
    "#df_spark.display()\n",
    "\n",
    "df_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalogo}.{bronze_db_name}.dm_cotacao_dolar\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Atividade2_land_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
